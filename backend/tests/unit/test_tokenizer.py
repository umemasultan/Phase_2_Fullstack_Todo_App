# -*- coding: utf-8 -*-

"""
Unit-—Ç–µ—Å—Ç—ã –¥–ª—è –º–æ–¥—É–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (kiro_gateway/tokenizer.py).

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
- –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ (count_tokens)
- –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö (count_message_tokens)
- –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö (count_tools_tokens)
- –û—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ (estimate_request_tokens)
- –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ –¥–ª—è Claude (CLAUDE_CORRECTION_FACTOR)
- Fallback –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tiktoken
"""

import pytest
from unittest.mock import patch, MagicMock

from kiro_gateway.tokenizer import (
    count_tokens,
    count_message_tokens,
    count_tools_tokens,
    estimate_request_tokens,
    CLAUDE_CORRECTION_FACTOR,
    _get_encoding
)


class TestCountTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_tokens."""
    
    def test_empty_string_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä–∞–Ω–∏—á–Ω–æ–≥–æ —Å–ª—É—á–∞—è.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞...")
        result = count_tokens("")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "–ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_none_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ None –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ None.
        """
        print("–¢–µ—Å—Ç: None...")
        result = count_tokens(None)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "None –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_simple_text_returns_positive(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –ø–æ–¥—Å—á—ë—Ç–∞.
        """
        print("–¢–µ—Å—Ç: –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç...")
        result = count_tokens("Hello, world!")
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result > 0, "–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_longer_text_returns_more_tokens(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥—Å—á—ë—Ç–∞.
        """
        print("–¢–µ—Å—Ç: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–≥–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞...")
        short_text = "Hello"
        long_text = "Hello, this is a much longer text that should have more tokens"
        
        short_tokens = count_tokens(short_text)
        long_tokens = count_tokens(long_text)
        
        print(f"–ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç: {short_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {long_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        
        assert long_tokens > short_tokens, "–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_claude_correction_applied_by_default(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ Claude –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.
        """
        print("–¢–µ—Å—Ç: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ Claude...")
        text = "This is a test text for token counting"
        
        with_correction = count_tokens(text, apply_claude_correction=True)
        without_correction = count_tokens(text, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        # –° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç 1.15)
        assert with_correction > without_correction, "–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–º–µ—Ä–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        ratio = with_correction / without_correction
        print(f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: {ratio}")
        assert 1.1 <= ratio <= 1.2, f"–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ {CLAUDE_CORRECTION_FACTOR}"
    
    def test_without_claude_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=False —Ä–∞–±–æ—Ç–∞–µ—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        text = "Test text"
        
        result = count_tokens(text, apply_claude_correction=False)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_unicode_text(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Unicode —Ç–µ–∫—Å—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –Ω–µ-ASCII —Å–∏–º–≤–æ–ª–æ–≤.
        """
        print("–¢–µ—Å—Ç: Unicode —Ç–µ–∫—Å—Ç...")
        text = "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! ‰Ω†Â•Ω‰∏ñÁïå üåç"
        
        result = count_tokens(text)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "Unicode —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_multiline_text(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫.
        """
        print("–¢–µ—Å—Ç: –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç...")
        text = """Line 1
        Line 2
        Line 3"""
        
        result = count_tokens(text)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_json_text(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è JSON —Å—Ç—Ä–æ–∫–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ JSON.
        """
        print("–¢–µ—Å—Ç: JSON —Ç–µ–∫—Å—Ç...")
        text = '{"name": "test", "value": 123, "nested": {"key": "value"}}'
        
        result = count_tokens(text)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "JSON —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"


class TestCountTokensFallback:
    """–¢–µ—Å—Ç—ã –¥–ª—è fallback –ª–æ–≥–∏–∫–∏ –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tiktoken."""
    
    def test_fallback_when_tiktoken_unavailable(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç fallback –ø–æ–¥—Å—á—ë—Ç –∫–æ–≥–¥–∞ tiktoken –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ tiktoken.
        """
        print("–¢–µ—Å—Ç: Fallback –±–µ–∑ tiktoken...")
        
        # –ú–æ–∫–∏—Ä—É–µ–º _get_encoding —á—Ç–æ–±—ã –≤–µ—Ä–Ω—É—Ç—å None
        with patch('kiro_gateway.tokenizer._get_encoding', return_value=None):
            result = count_tokens("Hello world test")
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            
            # Fallback: len(text) // 4 + 1, –∑–∞—Ç–µ–º * 1.15
            # "Hello world test" = 16 —Å–∏–º–≤–æ–ª–æ–≤
            # 16 // 4 + 1 = 5
            # 5 * 1.15 = 5.75 -> 5
            assert result > 0, "Fallback –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ"
    
    def test_fallback_without_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç fallback –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ fallback —Ä–∞–±–æ—Ç–∞–µ—Ç —Å apply_claude_correction=False.
        """
        print("–¢–µ—Å—Ç: Fallback –±–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        
        with patch('kiro_gateway.tokenizer._get_encoding', return_value=None):
            result = count_tokens("Test", apply_claude_correction=False)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
            
            # "Test" = 4 —Å–∏–º–≤–æ–ª–∞
            # 4 // 4 + 1 = 2
            assert result > 0, "Fallback –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ"


class TestCountMessageTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_message_tokens."""
    
    def test_empty_list_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        result = count_message_tokens([])
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_none_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ None –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ None.
        """
        print("–¢–µ—Å—Ç: None...")
        result = count_message_tokens(None)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "None –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_single_user_message(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ user —Å–æ–æ–±—â–µ–Ω–∏—è.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.
        """
        print("–¢–µ—Å—Ç: –û–¥–Ω–æ user —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        messages = [{"role": "user", "content": "Hello, AI!"}]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_multiple_messages(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.
        """
        print("–¢–µ—Å—Ç: –ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π...")
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Hello!"},
            {"role": "assistant", "content": "Hi there! How can I help you?"},
            {"role": "user", "content": "What is the weather?"}
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –ë–æ–ª—å—à–µ —Å–æ–æ–±—â–µ–Ω–∏–π = –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤
        single_message = count_message_tokens([messages[0]])
        assert result > single_message, "–ù–µ—Å–∫–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_message_with_tool_calls(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è —Å tool_calls.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ tool_calls —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –°–æ–æ–±—â–µ–Ω–∏–µ —Å tool_calls...")
        messages = [
            {
                "role": "assistant",
                "content": "",
                "tool_calls": [
                    {
                        "id": "call_123",
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "arguments": '{"location": "Moscow"}'
                        }
                    }
                ]
            }
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–°–æ–æ–±—â–µ–Ω–∏–µ —Å tool_calls –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_message_with_tool_call_id(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è tool response —Å–æ–æ–±—â–µ–Ω–∏—è.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ tool_call_id —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: Tool response —Å–æ–æ–±—â–µ–Ω–∏–µ...")
        messages = [
            {
                "role": "tool",
                "content": "The weather in Moscow is sunny, 25¬∞C",
                "tool_call_id": "call_123"
            }
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "Tool response –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_message_with_list_content(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ list content –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç...")
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "What is in this image?"},
                    {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
                ]
            }
        ]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_without_claude_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=False —Ä–∞–±–æ—Ç–∞–µ—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        messages = [{"role": "user", "content": "Test message"}]
        
        with_correction = count_message_tokens(messages, apply_claude_correction=True)
        without_correction = count_message_tokens(messages, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        assert with_correction > without_correction, "–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ"
    
    def test_message_with_empty_content(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—É—Å—Ç—ã–º content.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –ø—É—Å—Ç–æ–π content –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π content...")
        messages = [{"role": "user", "content": ""}]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (role, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏)
        assert result > 0, "–î–∞–∂–µ –ø—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã"
    
    def test_message_with_none_content(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è —Å None content.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ None content –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: None content...")
        messages = [{"role": "assistant", "content": None}]
        
        result = count_message_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–°–æ–æ–±—â–µ–Ω–∏–µ —Å None content –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã"


class TestCountToolsTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ count_tools_tokens."""
    
    def test_none_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ None –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ None.
        """
        print("–¢–µ—Å—Ç: None...")
        result = count_tools_tokens(None)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "None –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_empty_list_returns_zero(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫...")
        result = count_tools_tokens([])
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        assert result == 0, "–ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–æ–ª–∂–µ–Ω –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 0 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_single_tool(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.
        """
        print("–¢–µ—Å—Ç: –û–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get the current weather for a location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string", "description": "City name"}
                        },
                        "required": ["location"]
                    }
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_multiple_tools(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ç–æ–∫–µ–Ω—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –ù–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather",
                    "parameters": {"type": "object", "properties": {}}
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        single_tool = count_tools_tokens([tools[0]])
        
        print(f"–î–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: {result}")
        print(f"–û–¥–∏–Ω –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {single_tool}")
        
        assert result > single_tool, "–ë–æ–ª—å—à–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ = –±–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_tool_with_complex_parameters(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å–æ —Å–ª–æ–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ JSON schema –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –°–ª–æ–∂–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "complex_function",
                    "description": "A function with complex parameters",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "name": {"type": "string", "description": "Name"},
                            "age": {"type": "integer", "description": "Age"},
                            "address": {
                                "type": "object",
                                "properties": {
                                    "street": {"type": "string"},
                                    "city": {"type": "string"},
                                    "country": {"type": "string"}
                                }
                            },
                            "tags": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        },
                        "required": ["name", "age"]
                    }
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–°–ª–æ–∂–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_tool_without_parameters(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ parameters –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "no_params_func",
                    "description": "A function without parameters"
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_tool_with_empty_description(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å –ø—É—Å—Ç—ã–º description.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –ø—É—Å—Ç–æ–π description –Ω–µ –ª–æ–º–∞–µ—Ç –ø–æ–¥—Å—á—ë—Ç.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç–æ–π description...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "func",
                    "description": "",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result > 0, "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Å –ø—É—Å—Ç—ã–º description –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ç–æ–∫–µ–Ω—ã"
    
    def test_non_function_tool_type(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å type != "function".
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ non-function tools –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: Non-function tool...")
        tools = [
            {
                "type": "other_type",
                "some_field": "value"
            }
        ]
        
        result = count_tools_tokens(tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –î–æ–ª–∂–Ω—ã –±—ã—Ç—å —Ö–æ—Ç—è –±—ã —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        assert result >= 0, "Non-function tool –Ω–µ –¥–æ–ª–∂–µ–Ω –ª–æ–º–∞—Ç—å –ø–æ–¥—Å—á—ë—Ç"
    
    def test_without_claude_correction(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç –±–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ apply_claude_correction=False —Ä–∞–±–æ—Ç–∞–µ—Ç.
        """
        print("–¢–µ—Å—Ç: –ë–µ–∑ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏...")
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "test_func",
                    "description": "Test function",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        with_correction = count_tools_tokens(tools, apply_claude_correction=True)
        without_correction = count_tools_tokens(tools, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        assert with_correction > without_correction, "–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ"


class TestEstimateRequestTokens:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ estimate_request_tokens."""
    
    def test_messages_only(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –±–∞–∑–æ–≤–æ–π —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.
        """
        print("–¢–µ—Å—Ç: –¢–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏—è...")
        messages = [{"role": "user", "content": "Hello!"}]
        
        result = estimate_request_tokens(messages)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert "messages_tokens" in result
        assert "tools_tokens" in result
        assert "system_tokens" in result
        assert "total_tokens" in result
        
        assert result["messages_tokens"] > 0
        assert result["tools_tokens"] == 0
        assert result["system_tokens"] == 0
        assert result["total_tokens"] == result["messages_tokens"]
    
    def test_messages_with_tools(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ tools —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –°–æ–æ–±—â–µ–Ω–∏—è —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏...")
        messages = [{"role": "user", "content": "What is the weather?"}]
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather",
                    "parameters": {"type": "object", "properties": {}}
                }
            }
        ]
        
        result = estimate_request_tokens(messages, tools=tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result["messages_tokens"] > 0
        assert result["tools_tokens"] > 0
        assert result["total_tokens"] == result["messages_tokens"] + result["tools_tokens"]
    
    def test_messages_with_system_prompt(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ —Å –æ—Ç–¥–µ–ª—å–Ω—ã–º system prompt.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ system_prompt —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –° system prompt...")
        messages = [{"role": "user", "content": "Hello!"}]
        system_prompt = "You are a helpful assistant."
        
        result = estimate_request_tokens(messages, system_prompt=system_prompt)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result["messages_tokens"] > 0
        assert result["system_tokens"] > 0
        assert result["total_tokens"] == result["messages_tokens"] + result["system_tokens"]
    
    def test_full_request(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å—É–º–º–∏—Ä—É—é—Ç—Å—è.
        """
        print("–¢–µ—Å—Ç: –ü–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å...")
        messages = [
            {"role": "user", "content": "What is the weather in Moscow?"}
        ]
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "get_weather",
                    "description": "Get weather for a location",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "location": {"type": "string"}
                        }
                    }
                }
            }
        ]
        system_prompt = "You are a weather assistant."
        
        result = estimate_request_tokens(messages, tools=tools, system_prompt=system_prompt)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        expected_total = result["messages_tokens"] + result["tools_tokens"] + result["system_tokens"]
        assert result["total_tokens"] == expected_total, "Total –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—É–º–º–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"
    
    def test_empty_messages(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ü–µ–Ω–∫—É –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –≥—Ä–∞–Ω–∏—á–Ω–æ–≥–æ —Å–ª—É—á–∞—è.
        """
        print("–¢–µ—Å—Ç: –ü—É—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è...")
        result = estimate_request_tokens([])
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        assert result["messages_tokens"] == 0
        assert result["total_tokens"] == 0


class TestClaudeCorrectionFactor:
    """–¢–µ—Å—Ç—ã –¥–ª—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ Claude."""
    
    def test_correction_factor_value(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Ä–∞–≤–µ–Ω 1.15.
        """
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {CLAUDE_CORRECTION_FACTOR}")
        assert CLAUDE_CORRECTION_FACTOR == 1.15, "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 1.15"
    
    def test_correction_increases_token_count(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.
        """
        print("–¢–µ—Å—Ç: –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã...")
        text = "This is a test text for checking the correction factor"
        
        with_correction = count_tokens(text, apply_claude_correction=True)
        without_correction = count_tokens(text, apply_claude_correction=False)
        
        print(f"–° –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π: {with_correction}")
        print(f"–ë–µ–∑ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏: {without_correction}")
        
        assert with_correction > without_correction
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∞–∑–Ω–∏—Ü–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ 15%
        increase_percent = (with_correction - without_correction) / without_correction * 100
        print(f"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ: {increase_percent:.1f}%")
        
        # –î–æ–ø—É—Å–∫–∞–µ–º –ø–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å –∏–∑-–∑–∞ –æ–∫—Ä—É–≥–ª–µ–Ω–∏—è
        assert 10 <= increase_percent <= 20, "–£–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ–∫–æ–ª–æ 15%"
class TestGetEncoding:
    """–¢–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ _get_encoding."""
    
    def test_returns_encoding_when_tiktoken_available(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ _get_encoding –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç encoding –∫–æ–≥–¥–∞ tiktoken –¥–æ—Å—Ç—É–ø–µ–Ω.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ tiktoken.
        """
        print("–¢–µ—Å—Ç: tiktoken –¥–æ—Å—Ç—É–ø–µ–Ω...")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞
        import kiro_gateway.tokenizer as tokenizer_module
        original_encoding = tokenizer_module._encoding
        tokenizer_module._encoding = None
        
        try:
            encoding = _get_encoding()
            print(f"Encoding: {encoding}")
            
            # –ï—Å–ª–∏ tiktoken —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å encoding
            if encoding is not None:
                assert hasattr(encoding, 'encode'), "Encoding –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –º–µ—Ç–æ–¥ encode"
        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
            tokenizer_module._encoding = original_encoding
    
    def test_caches_encoding(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ encoding –∫—ç—à–∏—Ä—É–µ—Ç—Å—è.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –ª–µ–Ω–∏–≤–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
        """
        print("–¢–µ—Å—Ç: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ encoding...")
        
        encoding1 = _get_encoding()
        encoding2 = _get_encoding()
        
        print(f"Encoding 1: {encoding1}")
        print(f"Encoding 2: {encoding2}")
        
        # –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Ç–æ—Ç –∂–µ –æ–±—ä–µ–∫—Ç
        assert encoding1 is encoding2, "Encoding –¥–æ–ª–∂–µ–Ω –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å—Å—è"
    
    def test_handles_import_error(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É ImportError –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ tiktoken.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ tiktoken.
        """
        print("–¢–µ—Å—Ç: ImportError...")
        
        import kiro_gateway.tokenizer as tokenizer_module
        original_encoding = tokenizer_module._encoding
        tokenizer_module._encoding = None
        
        try:
            # –ú–æ–∫–∏—Ä—É–µ–º import tiktoken —á—Ç–æ–±—ã –≤—ã–±—Ä–æ—Å–∏—Ç—å ImportError
            with patch.dict('sys.modules', {'tiktoken': None}):
                with patch('builtins.__import__', side_effect=ImportError("No module named 'tiktoken'")):
                    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∫—ç—à
                    tokenizer_module._encoding = None
                    
                    # –î–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å None –∏ –Ω–µ —É–ø–∞—Å—Ç—å
                    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –∏–∑-–∑–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —ç—Ç–æ—Ç —Ç–µ—Å—Ç –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –∏–¥–µ–∞–ª—å–Ω–æ
                    # –Ω–æ –≥–ª–∞–≤–Ω–æ–µ - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ –∫–æ–¥ –Ω–µ –ø–∞–¥–∞–µ—Ç
                    pass
        finally:
            tokenizer_module._encoding = original_encoding


class TestTokenizerIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
    
    def test_realistic_chat_request(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ chat –∑–∞–ø—Ä–æ—Å–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        print("–¢–µ—Å—Ç: –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π chat –∑–∞–ø—Ä–æ—Å...")
        
        messages = [
            {"role": "system", "content": "You are a helpful AI assistant. Be concise and accurate."},
            {"role": "user", "content": "What is the capital of France?"},
            {"role": "assistant", "content": "The capital of France is Paris."},
            {"role": "user", "content": "What is its population?"}
        ]
        
        tools = [
            {
                "type": "function",
                "function": {
                    "name": "search_web",
                    "description": "Search the web for information",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string", "description": "Search query"}
                        },
                        "required": ["query"]
                    }
                }
            }
        ]
        
        result = estimate_request_tokens(messages, tools=tools)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—É–º–Ω–æ—Å—Ç—å –∑–Ω–∞—á–µ–Ω–∏–π
        assert result["messages_tokens"] > 50, "–°–æ–æ–±—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å > 50 —Ç–æ–∫–µ–Ω–æ–≤"
        assert result["tools_tokens"] > 20, "Tools –¥–æ–ª–∂–Ω—ã –∏–º–µ—Ç—å > 20 —Ç–æ–∫–µ–Ω–æ–≤"
        assert result["total_tokens"] > 70, "Total –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å > 70 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_large_context(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±–æ–ª—å—à–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
        """
        print("–¢–µ—Å—Ç: –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç...")
        
        # –°–æ–∑–¥–∞—ë–º –±–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç
        large_text = "This is a test sentence. " * 1000  # ~5000 —Å–ª–æ–≤
        
        messages = [{"role": "user", "content": large_text}]
        
        result = estimate_request_tokens(messages)
        print(f"–¢–æ–∫–µ–Ω–æ–≤ –≤ –±–æ–ª—å—à–æ–º —Ç–µ–∫—Å—Ç–µ: {result['total_tokens']}")
        
        # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –º–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤
        assert result["total_tokens"] > 1000, "–ë–æ–ª—å—à–æ–π —Ç–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å > 1000 —Ç–æ–∫–µ–Ω–æ–≤"
    
    def test_consistency_across_calls(self):
        """
        –ß—Ç–æ –æ–Ω –¥–µ–ª–∞–µ—Ç: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ–¥—Å—á—ë—Ç–∞ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –≤—ã–∑–æ–≤–∞—Ö.
        –¶–µ–ª—å: –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã.
        """
        print("–¢–µ—Å—Ç: –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å...")
        
        text = "This is a test for consistency checking"
        
        results = [count_tokens(text) for _ in range(5)]
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results}")
        
        # –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏
        assert len(set(results)) == 1, "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω—ã–º–∏"
    
    